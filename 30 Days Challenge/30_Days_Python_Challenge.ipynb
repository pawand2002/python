{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**ðŸš€ 30-Day Python Challenge: Let's do this.**"
      ],
      "metadata": {
        "id": "iBK1AR-wGEot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 1: Variables & Data Types."
      ],
      "metadata": {
        "id": "q72chgJjEB4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User Story: As a data engineer, I need to define and inspect various pieces of data from a new log file\n",
        "# so that I can understand its structure and prepare it for a data pipeline.\n",
        "\n",
        "# 1. Primitive Data Types\n",
        "\n",
        "# Integer: Used for counts, IDs, and other whole number values.\n",
        "# In this case, it's the number of records processed in a batch.\n",
        "batch_id = 101\n",
        "total_records_processed = 57892\n",
        "print(f\"Variable: 'batch_id', Value: {batch_id}, Type: {type(batch_id)}\")\n",
        "print(f\"Variable: 'total_records_processed', Value: {total_records_processed}, Type: {type(total_records_processed)}\\n\")\n",
        "\n",
        "# Float: Used for numerical data with a decimal point, such as timestamps or sizes.\n",
        "# Here, it's the duration of the data pipeline run in minutes.\n",
        "pipeline_duration_minutes = 12.45\n",
        "data_transfer_rate_mbps = 98.76\n",
        "print(f\"Variable: 'pipeline_duration_minutes', Value: {pipeline_duration_minutes}, Type: {type(pipeline_duration_minutes)}\")\n",
        "print(f\"Variable: 'data_transfer_rate_mbps', Value: {data_transfer_rate_mbps}, Type: {type(data_transfer_rate_mbps)}\\n\")\n",
        "\n",
        "# String: Used for text data, such as file paths, messages, or column names.\n",
        "log_file_name = \"data_ingestion_2023-10-26.log\"\n",
        "status_message = \"Ingestion process completed successfully.\"\n",
        "print(f\"Variable: 'log_file_name', Value: '{log_file_name}', Type: {type(log_file_name)}\")\n",
        "print(f\"Variable: 'status_message', Value: '{status_message}', Type: {type(status_message)}\\n\")\n",
        "\n",
        "# Boolean: Represents a true or false value, often used as flags.\n",
        "is_data_validated = True\n",
        "has_failed_records = False\n",
        "print(f\"Variable: 'is_data_validated', Value: {is_data_validated}, Type: {type(is_data_validated)}\")\n",
        "print(f\"Variable: 'has_failed_records', Value: {has_failed_records}, Type: {type(has_failed_records)}\\n\")\n",
        "\n",
        "# NoneType: Represents the absence of a value. This is a common way to initialize a variable\n",
        "# or indicate that a value is missing.\n",
        "error_code = None\n",
        "print(f\"Variable: 'error_code', Value: {error_code}, Type: {type(error_code)}\\n\")\n",
        "\n",
        "\n",
        "# 2. Collection Data Types\n",
        "\n",
        "# List: An ordered and mutable collection of items. Great for storing a sequence of objects.\n",
        "# In this scenario, it's a list of file paths to be processed.\n",
        "files_to_process = [\"/data/raw/file1.csv\", \"/data/raw/file2.csv\", \"/data/raw/file3.csv\"]\n",
        "print(f\"Variable: 'files_to_process', Value: {files_to_process}, Type: {type(files_to_process)}\")\n",
        "print(f\"First file in the list: {files_to_process[0]}\\n\")\n",
        "\n",
        "# Tuple: An ordered and immutable collection of items. Use it for fixed sequences.\n",
        "# This might be a fixed configuration like a database connection string parts.\n",
        "database_credentials = (\"mydb.hostname.com\", 5432, \"data_user\")\n",
        "print(f\"Variable: 'database_credentials', Value: {database_credentials}, Type: {type(database_credentials)}\")\n",
        "print(f\"Database hostname: {database_credentials[0]}\\n\")\n",
        "\n",
        "# Dictionary: A collection of key-value pairs. Ideal for representing structured records.\n",
        "# This represents metadata about a dataset.\n",
        "dataset_metadata = {\n",
        "    \"name\": \"sales_data\",\n",
        "    \"record_count\": 1205600,\n",
        "    \"last_updated\": \"2023-10-26T14:30:00Z\",\n",
        "    \"is_compressed\": True\n",
        "}\n",
        "print(f\"Variable: 'dataset_metadata', Value: {dataset_metadata}, Type: {type(dataset_metadata)}\")\n",
        "print(f\"The number of records is: {dataset_metadata['record_count']}\\n\")\n",
        "\n",
        "# Set: An unordered collection of unique items. Useful for removing duplicates.\n",
        "# This could be a set of unique user IDs from a log.\n",
        "unique_users = {\"user_a\", \"user_b\", \"user_c\", \"user_b\"}\n",
        "print(f\"Variable: 'unique_users', Value: {unique_users}, Type: {type(unique_users)}\")\n",
        "# Notice that 'user_b' only appears once in the set output.\n",
        "print(f\"Number of unique users: {len(unique_users)}\\n\")\n",
        "\n",
        "\n",
        "# 3. Simple Operations and Type Conversions\n",
        "\n",
        "# You can easily combine or convert between types.\n",
        "# For example, converting a string to a float.\n",
        "raw_record_size = \"1024.50\"\n",
        "normalized_size = float(raw_record_size)\n",
        "print(f\"Normalized record size: {normalized_size}, Type: {type(normalized_size)}\\n\")\n",
        "\n",
        "# Let's check a simple condition using a boolean.\n",
        "if is_data_validated:\n",
        "    print(\"Data validation passed. Ready for the next stage of the pipeline.\")\n",
        "else:\n",
        "    print(\"Data validation failed. Halting pipeline.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiY5XA-WEGQY",
        "outputId": "c18d4221-c720-41a1-bed6-de60f367ed78"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variable: 'batch_id', Value: 101, Type: <class 'int'>\n",
            "Variable: 'total_records_processed', Value: 57892, Type: <class 'int'>\n",
            "\n",
            "Variable: 'pipeline_duration_minutes', Value: 12.45, Type: <class 'float'>\n",
            "Variable: 'data_transfer_rate_mbps', Value: 98.76, Type: <class 'float'>\n",
            "\n",
            "Variable: 'log_file_name', Value: 'data_ingestion_2023-10-26.log', Type: <class 'str'>\n",
            "Variable: 'status_message', Value: 'Ingestion process completed successfully.', Type: <class 'str'>\n",
            "\n",
            "Variable: 'is_data_validated', Value: True, Type: <class 'bool'>\n",
            "Variable: 'has_failed_records', Value: False, Type: <class 'bool'>\n",
            "\n",
            "Variable: 'error_code', Value: None, Type: <class 'NoneType'>\n",
            "\n",
            "Variable: 'files_to_process', Value: ['/data/raw/file1.csv', '/data/raw/file2.csv', '/data/raw/file3.csv'], Type: <class 'list'>\n",
            "First file in the list: /data/raw/file1.csv\n",
            "\n",
            "Variable: 'database_credentials', Value: ('mydb.hostname.com', 5432, 'data_user'), Type: <class 'tuple'>\n",
            "Database hostname: mydb.hostname.com\n",
            "\n",
            "Variable: 'dataset_metadata', Value: {'name': 'sales_data', 'record_count': 1205600, 'last_updated': '2023-10-26T14:30:00Z', 'is_compressed': True}, Type: <class 'dict'>\n",
            "The number of records is: 1205600\n",
            "\n",
            "Variable: 'unique_users', Value: {'user_a', 'user_b', 'user_c'}, Type: <class 'set'>\n",
            "Number of unique users: 3\n",
            "\n",
            "Normalized record size: 1024.5, Type: <class 'float'>\n",
            "\n",
            "Data validation passed. Ready for the next stage of the pipeline.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 2: Operators"
      ],
      "metadata": {
        "id": "kDc-FxkBE6zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User Story: As a data engineer, I need to perform calculations and comparisons\n",
        "# on a data batch to check its integrity and determine its final status.\n",
        "\n",
        "# 1. Defining Our Data\n",
        "# Imagine these values come from a data pipeline log.\n",
        "records_in_batch = 1000\n",
        "records_processed = 980\n",
        "error_records = 20\n",
        "processing_time_sec = 125\n",
        "\n",
        "# 2. Arithmetic Operators\n",
        "\n",
        "# Calculate the percentage of processed records.\n",
        "# Note: The result will be a float.\n",
        "processing_percentage = (records_processed / records_in_batch) * 100\n",
        "print(f\"Processing Percentage: {processing_percentage:.2f}%\\n\") # The .2f formats the float to 2 decimal places.\n",
        "\n",
        "# Calculate the average time to process each record.\n",
        "avg_time_per_record_sec = processing_time_sec / records_processed\n",
        "print(f\"Average time per record: {avg_time_per_record_sec:.4f} seconds\\n\")\n",
        "\n",
        "# Use floor division to find how many full batches of 500 records were processed.\n",
        "# This discards the remainder.\n",
        "full_500_record_batches = records_processed // 500\n",
        "print(f\"Number of full 500-record batches processed: {full_500_record_batches}\\n\")\n",
        "\n",
        "# Use modulus to find the number of remaining records.\n",
        "remaining_records = records_processed % 500\n",
        "print(f\"Number of remaining records after full batches: {remaining_records}\\n\")\n",
        "\n",
        "\n",
        "# 3. Comparison Operators\n",
        "\n",
        "# Check if the number of processed records is equal to the expected amount.\n",
        "expected_records = 980\n",
        "is_processed_count_correct = (records_processed == expected_records)\n",
        "print(f\"Is processed count correct? {is_processed_count_correct}\")\n",
        "\n",
        "# Check if there are any error records.\n",
        "has_errors = (error_records > 0)\n",
        "print(f\"Does the batch have errors? {has_errors}\")\n",
        "\n",
        "# Check if the processing time is within a 2-minute threshold (120 seconds).\n",
        "is_within_time_limit = (processing_time_sec <= 120)\n",
        "print(f\"Is processing time within the limit? {is_within_time_limit}\\n\")\n",
        "\n",
        "\n",
        "# 4. Logical Operators\n",
        "\n",
        "# Combine conditions to make a final decision about the batch status.\n",
        "# The `and` operator requires both conditions to be True.\n",
        "is_batch_successful = is_processed_count_correct and not has_errors\n",
        "print(f\"Is the batch considered successful? {is_batch_successful}\")\n",
        "\n",
        "# The `or` operator is True if at least one condition is True.\n",
        "is_critical_issue = has_errors or (processing_time_sec > 300)\n",
        "print(f\"Is there a critical issue with the batch? {is_critical_issue}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pk25_ALFI-v",
        "outputId": "fbdeb9d5-7c4a-40ed-b093-b39756fa1aa3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Percentage: 98.00%\n",
            "\n",
            "Average time per record: 0.1276 seconds\n",
            "\n",
            "Number of full 500-record batches processed: 1\n",
            "\n",
            "Number of remaining records after full batches: 480\n",
            "\n",
            "Is processed count correct? True\n",
            "Does the batch have errors? True\n",
            "Is processing time within the limit? False\n",
            "\n",
            "Is the batch considered successful? False\n",
            "Is there a critical issue with the batch? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 3 : Conditional Statements"
      ],
      "metadata": {
        "id": "j2SezA9yF-vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User Story: As a data engineer, I need to check the status of a data transfer\n",
        "# and execute different actions based on whether it succeeded, failed, or is still in progress.\n",
        "\n",
        "# 1. Defining Our Data\n",
        "# Imagine these values are read from a pipeline monitoring system.\n",
        "transfer_status = \"succeeded\"  # Can be \"succeeded\", \"failed\", or \"in progress\"\n",
        "file_size_gb = 5.7\n",
        "transfer_speed_mbps = 85.0\n",
        "\n",
        "# 2. Basic 'if', 'elif', 'else' Statement\n",
        "\n",
        "# We check the status to determine the next step in our pipeline.\n",
        "if transfer_status == \"succeeded\":\n",
        "    print(\"Transfer successful. Proceeding to data validation phase.\")\n",
        "elif transfer_status == \"failed\":\n",
        "    print(\"Transfer failed. Notifying the on-call team and retrying the job.\")\n",
        "else:\n",
        "    print(\"Transfer is still in progress. Monitoring for completion...\")\n",
        "\n",
        "\n",
        "# 3. Nested 'if' and Combining Conditions\n",
        "\n",
        "# We can add more specific checks inside a condition.\n",
        "if transfer_status == \"succeeded\":\n",
        "    # Using a comparison and logical operator\n",
        "    if file_size_gb > 5 and transfer_speed_mbps < 100:\n",
        "        print(\"Large file transferred successfully, but speed was below optimal. Need to investigate.\")\n",
        "    else:\n",
        "        print(\"Large file transferred successfully with optimal speed.\")\n",
        "elif transfer_status == \"failed\":\n",
        "    # Another condition check\n",
        "    if file_size_gb > 10:\n",
        "        print(\"Large file transfer failed. Escalating issue to a senior engineer.\")\n",
        "    else:\n",
        "        print(\"File transfer failed. Retrying with standard procedure.\")\n",
        "else:\n",
        "    print(\"No further action needed at this time.\")\n",
        "\n",
        "\n",
        "# 4. Using Booleans and Conditional Expressions (Ternary Operator)\n",
        "\n",
        "# A boolean flag can simplify conditional logic.\n",
        "is_transfer_complete = transfer_status == \"succeeded\"\n",
        "if is_transfer_complete:\n",
        "    print(\"\\nTransfer completed flag is True.\")\n",
        "else:\n",
        "    print(\"\\nTransfer completed flag is False.\")\n",
        "\n",
        "# A more concise way to write a simple conditional.\n",
        "message = \"Ready for processing\" if is_transfer_complete else \"Awaiting transfer completion\"\n",
        "print(f\"Status message: {message}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngjrgdOLFvNB",
        "outputId": "17068809-4ff9-41df-fc70-77de84c257fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transfer successful. Proceeding to data validation phase.\n",
            "Large file transferred successfully, but speed was below optimal. Need to investigate.\n",
            "\n",
            "Transfer completed flag is True.\n",
            "Status message: Ready for processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 4 Loops: for & while\n"
      ],
      "metadata": {
        "id": "5pjmS7VoGbh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User Story: As a data engineer, I need to perform a task a specific number of times,\n",
        "# such as processing a set number of records or retrying a failed job.\n",
        "\n",
        "# 1. 'for' Loop: Iterating a specific number of times\n",
        "\n",
        "print(\"--- Using 'for' loop to process a specific number of records ---\")\n",
        "# This loop will run 5 times, with the 'record_number' variable going from 0 to 4.\n",
        "for record_number in range(5):\n",
        "    print(f\"   -> Processing record number {record_number + 1}...\")\n",
        "\n",
        "    # We can use an if statement inside the loop to simulate a check.\n",
        "    if record_number == 2:\n",
        "        print(\"      -> A special condition was met for this record. Taking action.\")\n",
        "\n",
        "print(\"\\nAll records processed using the 'for' loop.\\n\")\n",
        "\n",
        "\n",
        "# 2. 'while' Loop: Looping based on a condition\n",
        "\n",
        "# This is useful for monitoring a state, like waiting for a queue to be empty.\n",
        "records_to_process = 5\n",
        "processed_count = 0\n",
        "\n",
        "print(\"--- Using 'while' loop to monitor a queue ---\")\n",
        "print(f\"Initial records in queue: {records_to_process}\")\n",
        "\n",
        "while processed_count < records_to_process:\n",
        "    processed_count += 1\n",
        "    print(f\"   -> Processed record {processed_count}. Records remaining: {records_to_process - processed_count}\")\n",
        "    # In a real pipeline, this would involve calling a function to process a record.\n",
        "\n",
        "print(\"\\nAll records have been processed. The queue is empty.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-Fi-kMjGiwT",
        "outputId": "25225df3-2e96-423d-d1cd-3e623846e9cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Using 'for' loop to process a specific number of records ---\n",
            "   -> Processing record number 1...\n",
            "   -> Processing record number 2...\n",
            "   -> Processing record number 3...\n",
            "      -> A special condition was met for this record. Taking action.\n",
            "   -> Processing record number 4...\n",
            "   -> Processing record number 5...\n",
            "\n",
            "All records processed using the 'for' loop.\n",
            "\n",
            "--- Using 'while' loop to monitor a queue ---\n",
            "Initial records in queue: 5\n",
            "   -> Processed record 1. Records remaining: 4\n",
            "   -> Processed record 2. Records remaining: 3\n",
            "   -> Processed record 3. Records remaining: 2\n",
            "   -> Processed record 4. Records remaining: 1\n",
            "   -> Processed record 5. Records remaining: 0\n",
            "\n",
            "All records have been processed. The queue is empty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# User Story: As a data engineer, I need to automate a daily data ingestion process\n",
        "# and build a retry mechanism for failed jobs.\n",
        "\n",
        "# 1. 'for' Loop: Simulating a daily scheduled job\n",
        "\n",
        "print(\"--- Using 'for' loop to simulate a daily ingestion schedule ---\")\n",
        "# We want to run a daily ingestion job for a period of 7 days.\n",
        "# The 'day_number' variable will be an integer from 1 to 7.\n",
        "for day_number in range(1, 8):\n",
        "    print(f\"   -> Starting ingestion job for Day {day_number}...\")\n",
        "\n",
        "    # We can use a simple conditional check to simulate a successful or failed run.\n",
        "    # In a real pipeline, this would be determined by a function's return value.\n",
        "    if day_number == 4:\n",
        "        print(f\"      -> Ingestion on Day {day_number} failed. Manual review required.\")\n",
        "    else:\n",
        "        print(f\"      -> Ingestion on Day {day_number} completed successfully.\")\n",
        "\n",
        "print(\"\\nDaily ingestion schedule for the week has finished.\\n\")\n",
        "\n",
        "\n",
        "# 2. 'while' Loop: Building a retry mechanism\n",
        "\n",
        "# This is a common pattern for handling temporary issues, like a network outage.\n",
        "# We will simulate a job that might fail and then retry up to a maximum number of attempts.\n",
        "is_successful = False\n",
        "retry_attempts = 0\n",
        "max_attempts = 3\n",
        "\n",
        "print(\"--- Using 'while' loop for a job retry mechanism ---\")\n",
        "\n",
        "# The loop continues as long as the job is not successful AND we haven't\n",
        "# exceeded our maximum number of retries.\n",
        "while not is_successful and retry_attempts < max_attempts:\n",
        "    retry_attempts += 1\n",
        "    print(f\"   -> Attempting to connect to data source... (Attempt {retry_attempts} of {max_attempts})\")\n",
        "\n",
        "    # In a real scenario, a function call here would return True or False.\n",
        "    # We'll simulate a failure on the first two attempts.\n",
        "    if retry_attempts < max_attempts:\n",
        "        print(\"      -> Connection failed. Retrying...\")\n",
        "    else:\n",
        "        print(\"      -> Connection successful! Data transfer can begin.\")\n",
        "        is_successful = True\n",
        "\n",
        "# After the loop, we can check the final status.\n",
        "if is_successful:\n",
        "    print(\"\\nJob completed successfully after retries.\")\n",
        "else:\n",
        "    print(\"\\nJob failed after all retry attempts. Notifying engineer.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pMm8Nm-HJ5v",
        "outputId": "06309648-ca34-43b0-b429-ae1f3f8189df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Using 'for' loop to simulate a daily ingestion schedule ---\n",
            "   -> Starting ingestion job for Day 1...\n",
            "      -> Ingestion on Day 1 completed successfully.\n",
            "   -> Starting ingestion job for Day 2...\n",
            "      -> Ingestion on Day 2 completed successfully.\n",
            "   -> Starting ingestion job for Day 3...\n",
            "      -> Ingestion on Day 3 completed successfully.\n",
            "   -> Starting ingestion job for Day 4...\n",
            "      -> Ingestion on Day 4 failed. Manual review required.\n",
            "   -> Starting ingestion job for Day 5...\n",
            "      -> Ingestion on Day 5 completed successfully.\n",
            "   -> Starting ingestion job for Day 6...\n",
            "      -> Ingestion on Day 6 completed successfully.\n",
            "   -> Starting ingestion job for Day 7...\n",
            "      -> Ingestion on Day 7 completed successfully.\n",
            "\n",
            "Daily ingestion schedule for the week has finished.\n",
            "\n",
            "--- Using 'while' loop for a job retry mechanism ---\n",
            "   -> Attempting to connect to data source... (Attempt 1 of 3)\n",
            "      -> Connection failed. Retrying...\n",
            "   -> Attempting to connect to data source... (Attempt 2 of 3)\n",
            "      -> Connection failed. Retrying...\n",
            "   -> Attempting to connect to data source... (Attempt 3 of 3)\n",
            "      -> Connection successful! Data transfer can begin.\n",
            "\n",
            "Job completed successfully after retries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# User Story: As a data engineer, I sometimes need to perform mathematical\n",
        "# calculations on scalar values, such as calculating the number of possible\n",
        "# combinations for a data batch (factorial) or generating a sequence for a\n",
        "# specific data pattern (Fibonacci).\n",
        "\n",
        "# --- Part 1: Calculating a Factorial ---\n",
        "# A factorial is the product of all positive integers up to a number.\n",
        "# For example, 5! = 5 * 4 * 3 * 2 * 1 = 120.\n",
        "# We will use a 'for' loop to perform this calculation.\n",
        "\n",
        "print(\"--- Calculating Factorial ---\")\n",
        "# The number for which we want to calculate the factorial.\n",
        "number = 7\n",
        "\n",
        "# We initialize a variable to hold the result.\n",
        "factorial_result = 1\n",
        "\n",
        "# We loop from the number down to 1. The 'range' function is exclusive of the end value.\n",
        "for i in range(1, number + 1):\n",
        "    factorial_result = factorial_result * i\n",
        "    print(f\"  -> Intermediate result for {i}: {factorial_result}\")\n",
        "\n",
        "print(f\"\\nThe factorial of {number} is: {factorial_result}\\n\")\n",
        "\n",
        "\n",
        "# --- Part 2: Generating the Fibonacci Sequence ---\n",
        "# The Fibonacci sequence is a series where each number is the sum of the two\n",
        "# preceding ones, starting from 0 and 1. (0, 1, 1, 2, 3, 5, 8, ...)\n",
        "# We will use a 'while' loop to generate this sequence up to a certain number of terms.\n",
        "\n",
        "print(\"--- Generating Fibonacci Sequence ---\")\n",
        "# The number of terms we want to generate.\n",
        "num_terms = 10\n",
        "\n",
        "# Initialize the first two terms of the sequence.\n",
        "a = 0\n",
        "b = 1\n",
        "\n",
        "# A counter to track how many terms we have printed.\n",
        "count = 0\n",
        "\n",
        "# The 'while' loop will continue as long as our count is less than the desired number of terms.\n",
        "while count < num_terms:\n",
        "    print(a, end=\" \") # 'end=\" \"' keeps the output on the same line.\n",
        "\n",
        "    # Calculate the next term.\n",
        "    next_term = a + b\n",
        "\n",
        "    # Update the values for the next iteration.\n",
        "    a = b\n",
        "    b = next_term\n",
        "\n",
        "    # Increment the counter.\n",
        "    count += 1\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# --- Part 3: Using 'sum' for Aggregation ---\n",
        "# User Story: A data engineer needs to calculate the total number of records\n",
        "# processed across several parallel jobs.\n",
        "\n",
        "print(\"--- Calculating Sum of Records ---\")\n",
        "# For now, we'll represent job records as individual numbers.\n",
        "# Later, you will learn to use lists for this, but for now, we'll use a simple loop.\n",
        "job_1_records = 150\n",
        "job_2_records = 220\n",
        "job_3_records = 95\n",
        "job_4_records = 300\n",
        "\n",
        "total_records = sum([job_1_records, job_2_records, job_3_records, job_4_records])\n",
        "print(f\"The total number of records processed is: {total_records}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# --- Part 4: Using 'min' and 'max' for Data Range ---\n",
        "# User Story: A data engineer needs to find the smallest and largest file sizes\n",
        "# within a batch to identify anomalies.\n",
        "\n",
        "print(\"--- Finding Min/Max File Size ---\")\n",
        "# Similar to the sum, we will use individual variables to simulate file sizes.\n",
        "# The 'min()' and 'max()' functions can find the smallest and largest values.\n",
        "file_size_1 = 1024\n",
        "file_size_2 = 512\n",
        "file_size_3 = 2048\n",
        "file_size_4 = 1536\n",
        "\n",
        "min_size = min(file_size_1, file_size_2, file_size_3, file_size_4)\n",
        "max_size = max(file_size_1, file_size_2, file_size_3, file_size_4)\n",
        "\n",
        "print(f\"The minimum file size is: {min_size} KB\")\n",
        "print(f\"The maximum file size is: {max_size} KB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shf4f3G4HgDt",
        "outputId": "571c4349-9620-4dd6-c7ad-ee6c83ba4388"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Calculating Factorial ---\n",
            "  -> Intermediate result for 1: 1\n",
            "  -> Intermediate result for 2: 2\n",
            "  -> Intermediate result for 3: 6\n",
            "  -> Intermediate result for 4: 24\n",
            "  -> Intermediate result for 5: 120\n",
            "  -> Intermediate result for 6: 720\n",
            "  -> Intermediate result for 7: 5040\n",
            "\n",
            "The factorial of 7 is: 5040\n",
            "\n",
            "--- Generating Fibonacci Sequence ---\n",
            "0 1 1 2 3 5 8 13 21 34 \n",
            "\n",
            "--- Calculating Sum of Records ---\n",
            "The total number of records processed is: 765\n",
            "\n",
            "\n",
            "--- Finding Min/Max File Size ---\n",
            "The minimum file size is: 512 KB\n",
            "The maximum file size is: 2048 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculator Application"
      ],
      "metadata": {
        "id": "9zI-B_esHyNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User Story: As a data engineer, I often need to perform quick calculations to\n",
        "# validate data or check values in a pipeline. A simple, ad-hoc calculator\n",
        "# helps me quickly get these results without a spreadsheet.\n",
        "\n",
        "# --- Part 1: Getting User Input ---\n",
        "# We use the 'input()' function to get data from the user.\n",
        "# IMPORTANT: The input() function always returns a string. We must\n",
        "# convert it to a number (float) to perform mathematical operations.\n",
        "\n",
        "print(\"--- Simple Python Calculator ---\")\n",
        "\n",
        "# We use a try-except block to handle cases where the user enters non-numeric input.\n",
        "try:\n",
        "    # Get the first number from the user.\n",
        "    num1_str = input(\"Enter the first number: \")\n",
        "    num1 = float(num1_str)\n",
        "\n",
        "    # Get the second number from the user.\n",
        "    num2_str = input(\"Enter the second number: \")\n",
        "    num2 = float(num2_str)\n",
        "\n",
        "    # Get the desired operation.\n",
        "    operation = input(\"Enter the operation (+, -, *, /): \")\n",
        "\n",
        "    # --- Part 2: Performing the Calculation ---\n",
        "    # We use conditional statements (if/elif/else) to choose the correct operation.\n",
        "    if operation == '+':\n",
        "        result = num1 + num2\n",
        "        print(f\"Result: {num1} + {num2} = {result}\")\n",
        "    elif operation == '-':\n",
        "        result = num1 - num2\n",
        "        print(f\"Result: {num1} - {num2} = {result}\")\n",
        "    elif operation == '*':\n",
        "        result = num1 * num2\n",
        "        print(f\"Result: {num1} * {num2} = {result}\")\n",
        "    elif operation == '/':\n",
        "        # We need to handle division by zero separately to avoid an error.\n",
        "        if num2 == 0:\n",
        "            print(\"Error: Cannot divide by zero.\")\n",
        "        else:\n",
        "            result = num1 / num2\n",
        "            print(f\"Result: {num1} / {num2} = {result}\")\n",
        "    else:\n",
        "        # This handles any invalid operation input.\n",
        "        print(\"Error: Invalid operation.\")\n",
        "\n",
        "# This block catches the error if the user input for the numbers cannot be converted to a float.\n",
        "except ValueError:\n",
        "    print(\"Error: Please enter valid numbers.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkBq9PZ2H1Ve",
        "outputId": "ad94be85-4871-490e-e07c-bdc164fc3224"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Simple Python Calculator ---\n",
            "Enter the first number: 45\n",
            "Enter the second number: 69\n",
            "Enter the operation (+, -, *, /): +\n",
            "Result: 45.0 + 69.0 = 114.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# User Story: As a data engineer, I need to perform basic data quality checks\n",
        "# on a batch of incoming records. I need to iterate through each record,\n",
        "# check if it meets certain criteria, and then perform a simple transformation\n",
        "# or aggregation based on the results.\n",
        "\n",
        "# This application demonstrates the core concepts learned through Day 4:\n",
        "# - Variables and Data Types\n",
        "# - Operators and Expressions\n",
        "# - Conditional Statements (if/elif/else)\n",
        "# - Loops (for)\n",
        "\n",
        "# --- Part 1: Setting up the initial variables and parameters ---\n",
        "# We'll simulate a data pipeline by setting up some key variables.\n",
        "batch_size = 20  # Total number of records to process\n",
        "threshold = 10   # A value used for a data quality check\n",
        "valid_records = 0\n",
        "invalid_records = 0\n",
        "\n",
        "print(f\"--- Data Quality Check: Processing a batch of {batch_size} records ---\")\n",
        "print(f\"Threshold for valid data: > {threshold}\\n\")\n",
        "\n",
        "\n",
        "# --- Part 2: Looping through the data records ---\n",
        "# We use a 'for' loop to simulate iterating over each record in the batch.\n",
        "# The 'range(1, batch_size + 1)' generates numbers from 1 to 20, representing\n",
        "# our record IDs.\n",
        "for record_id in range(1, batch_size + 1):\n",
        "    # Simulate a value for each record. For this example, we'll use a simple calculation.\n",
        "    # In a real pipeline, this value would be read from a file or database.\n",
        "    # Here, the value is just the record ID itself.\n",
        "    record_value = record_id\n",
        "\n",
        "    # --- Part 3: Performing conditional checks ---\n",
        "    # We use 'if/elif/else' to apply our data quality rules.\n",
        "    if record_value > threshold:\n",
        "        # If the value is above the threshold, it's considered valid.\n",
        "        print(f\"Record {record_id}: Value of {record_value} is VALID.\")\n",
        "        # We increment the count of valid records.\n",
        "        valid_records = valid_records + 1\n",
        "    elif record_value == threshold:\n",
        "        # If the value is exactly the threshold, it's a special case.\n",
        "        print(f\"Record {record_id}: Value of {record_value} is a BORDERLINE case.\")\n",
        "    else:\n",
        "        # If the value is below the threshold, it's considered invalid.\n",
        "        print(f\"Record {record_id}: Value of {record_value} is INVALID.\")\n",
        "        # We increment the count of invalid records.\n",
        "        invalid_records = invalid_records + 1\n",
        "\n",
        "\n",
        "# --- Part 4: Final Summary and Aggregation ---\n",
        "# After the loop finishes, we print a summary of the processed batch.\n",
        "# This part uses the aggregated variables and a final conditional statement.\n",
        "print(\"\\n--- Batch Processing Summary ---\")\n",
        "print(f\"Total Records Processed: {valid_records + invalid_records}\")\n",
        "print(f\"Valid Records: {valid_records}\")\n",
        "print(f\"Invalid Records: {invalid_records}\")\n",
        "\n",
        "# Use a final check to see if the batch passed or failed quality standards.\n",
        "if valid_records > invalid_records:\n",
        "    print(\"\\nCONCLUSION: The batch passed the quality check.\")\n",
        "else:\n",
        "    print(\"\\nCONCLUSION: The batch failed the quality check. Further investigation needed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM7LkxXWIOFg",
        "outputId": "7b7192e4-de8b-4b53-8690-f84dcce5ef4b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Data Quality Check: Processing a batch of 20 records ---\n",
            "Threshold for valid data: > 10\n",
            "\n",
            "Record 1: Value of 1 is INVALID.\n",
            "Record 2: Value of 2 is INVALID.\n",
            "Record 3: Value of 3 is INVALID.\n",
            "Record 4: Value of 4 is INVALID.\n",
            "Record 5: Value of 5 is INVALID.\n",
            "Record 6: Value of 6 is INVALID.\n",
            "Record 7: Value of 7 is INVALID.\n",
            "Record 8: Value of 8 is INVALID.\n",
            "Record 9: Value of 9 is INVALID.\n",
            "Record 10: Value of 10 is a BORDERLINE case.\n",
            "Record 11: Value of 11 is VALID.\n",
            "Record 12: Value of 12 is VALID.\n",
            "Record 13: Value of 13 is VALID.\n",
            "Record 14: Value of 14 is VALID.\n",
            "Record 15: Value of 15 is VALID.\n",
            "Record 16: Value of 16 is VALID.\n",
            "Record 17: Value of 17 is VALID.\n",
            "Record 18: Value of 18 is VALID.\n",
            "Record 19: Value of 19 is VALID.\n",
            "Record 20: Value of 20 is VALID.\n",
            "\n",
            "--- Batch Processing Summary ---\n",
            "Total Records Processed: 19\n",
            "Valid Records: 10\n",
            "Invalid Records: 9\n",
            "\n",
            "CONCLUSION: The batch passed the quality check.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 5 Tuples & Sets"
      ],
      "metadata": {
        "id": "yABRoPjcImCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User Story: As a data engineer, I need to handle data that should not be\n",
        "# changed (immutable), like database connection details, or filter for unique\n",
        "# values in a dataset, like a list of distinct users.\n",
        "\n",
        "# --- Part 1: Tuples - Immutable Data ---\n",
        "# Tuples are ordered collections of items, but they are immutable, which means\n",
        "# you cannot change, add, or remove items after creation. They are often used\n",
        "# for fixed data, like coordinates or configuration settings.\n",
        "\n",
        "print(\"--- Using Tuples for Immutable Data ---\")\n",
        "# Example: A tuple to store database connection information.\n",
        "# The host, port, and user should not be accidentally changed.\n",
        "db_connection = (\"localhost\", 5432, \"data_pipeline_user\")\n",
        "print(f\"Database connection details: {db_connection}\")\n",
        "\n",
        "# You can access elements by their index, just like a list.\n",
        "print(f\"Database host: {db_connection[0]}\")\n",
        "print(f\"Database port: {db_connection[1]}\")\n",
        "\n",
        "# If you try to change a value, it will result in a TypeError.\n",
        "# Try uncommenting the line below to see the error:\n",
        "# db_connection[1] = 5433\n",
        "\n",
        "# --- Part 2: Sets - Unique Data ---\n",
        "# Sets are unordered collections of unique items. They are incredibly\n",
        "# efficient for tasks like removing duplicate values or checking for\n",
        "# membership.\n",
        "\n",
        "print(\"\\n--- Using Sets for Unique Data ---\")\n",
        "# Example: A list of user IDs from a log file, which may have duplicates.\n",
        "user_ids_raw = [\"u101\", \"u102\", \"u103\", \"u102\", \"u104\", \"u101\"]\n",
        "print(f\"Original user IDs (with duplicates): {user_ids_raw}\")\n",
        "\n",
        "# To get the unique user IDs, we can simply convert the list to a set.\n",
        "unique_user_ids = set(user_ids_raw)\n",
        "print(f\"Unique user IDs (as a set): {unique_user_ids}\")\n",
        "\n",
        "# You can also perform set operations. For example, to find users in one batch\n",
        "# but not another, or to find the intersection of two sets of users.\n",
        "batch_1_users = {\"u101\", \"u102\", \"u103\"}\n",
        "batch_2_users = {\"u102\", \"u104\", \"u105\"}\n",
        "\n",
        "# Find users common to both batches.\n",
        "common_users = batch_1_users.intersection(batch_2_users)\n",
        "print(f\"Common users between batch 1 and 2: {common_users}\")\n",
        "\n",
        "# Find users who are in batch 1 but not in batch 2.\n",
        "new_users = batch_1_users.difference(batch_2_users)\n",
        "print(f\"Users only in batch 1: {new_users}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S21F7FM1Isk5",
        "outputId": "baa93d15-156d-4e94-b4b8-43c52561f7b7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Using Tuples for Immutable Data ---\n",
            "Database connection details: ('localhost', 5432, 'data_pipeline_user')\n",
            "Database host: localhost\n",
            "Database port: 5432\n",
            "\n",
            "--- Using Sets for Unique Data ---\n",
            "Original user IDs (with duplicates): ['u101', 'u102', 'u103', 'u102', 'u104', 'u101']\n",
            "Unique user IDs (as a set): {'u102', 'u103', 'u104', 'u101'}\n",
            "Common users between batch 1 and 2: {'u102'}\n",
            "Users only in batch 1: {'u103', 'u101'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 5 : Tuples Practice"
      ],
      "metadata": {
        "id": "3_rwOYQaL8rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User Story: As a data engineer, I have a batch of fixed metadata for\n",
        "# each job run, such as the start time, end time, and status. This data\n",
        "# should not change.\n",
        "\n",
        "# --- Part 1: Immutable Data with Tuples ---\n",
        "# A tuple is perfect for storing data that should remain constant.\n",
        "print(\"--- Tuple Practice: Job Run Metadata ---\")\n",
        "# Example: A tuple representing a single job's metadata.\n",
        "job_run_1 = (\"job_id_abc\", \"2024-05-18T10:00:00Z\", \"SUCCESS\")\n",
        "job_run_2 = (\"job_id_xyz\", \"2024-05-18T11:30:00Z\", \"FAILED\")\n",
        "print(f\"Job 1 Metadata: {job_run_1}\")\n",
        "print(f\"Job 2 Metadata: {job_run_2}\")\n",
        "\n",
        "# Accessing elements is straightforward using indexing.\n",
        "print(f\"Status of Job 2: {job_run_2[2]}\")\n",
        "\n",
        "# --- Part 2: Tuple Functions for Data Analysis ---\n",
        "# Tuples have a few methods that are useful for querying information\n",
        "# without changing the data.\n",
        "\n",
        "print(\"\\n--- Tuple Functions ---\")\n",
        "# Example: A tuple of error codes from a batch log.\n",
        "error_codes = (200, 404, 500, 200, 403, 500, 500)\n",
        "print(f\"Error Codes Tuple: {error_codes}\")\n",
        "\n",
        "# count(): Counts how many times an item appears.\n",
        "print(f\"Number of 200 (success) codes: {error_codes.count(200)}\")\n",
        "print(f\"Number of 500 (server error) codes: {error_codes.count(500)}\")\n",
        "\n",
        "# index(): Finds the first position of a specific item.\n",
        "# This is useful for finding the first occurrence of an error.\n",
        "try:\n",
        "    first_server_error_index = error_codes.index(500)\n",
        "    print(f\"First 500 error occurred at index: {first_server_error_index}\")\n",
        "except ValueError:\n",
        "    print(\"500 error was not found in the tuple.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOps-ZcRMCtM",
        "outputId": "34971bb6-3479-4bd4-f9cf-5d47e8c5d4a9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tuple Practice: Job Run Metadata ---\n",
            "Job 1 Metadata: ('job_id_abc', '2024-05-18T10:00:00Z', 'SUCCESS')\n",
            "Job 2 Metadata: ('job_id_xyz', '2024-05-18T11:30:00Z', 'FAILED')\n",
            "Status of Job 2: FAILED\n",
            "\n",
            "--- Tuple Functions ---\n",
            "Error Codes Tuple: (200, 404, 500, 200, 403, 500, 500)\n",
            "Number of 200 (success) codes: 2\n",
            "Number of 500 (server error) codes: 3\n",
            "First 500 error occurred at index: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sets Practice"
      ],
      "metadata": {
        "id": "-iMR2RBVMK2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User Story: As a data engineer, I need to identify unique user IDs from a\n",
        "# large, raw log file. I also need to compare different log files to see which\n",
        "# users they have in common or which are exclusive to one file.\n",
        "\n",
        "# --- Part 1: Unique Data with Sets ---\n",
        "# Sets are unordered collections that only store unique values. They are\n",
        "# extremely efficient for removing duplicates.\n",
        "\n",
        "print(\"--- Set Practice: Finding Unique Users ---\")\n",
        "# Example: A list of user IDs from a raw log, containing duplicates.\n",
        "raw_user_log = [\"user_1\", \"user_2\", \"user_1\", \"user_3\", \"user_4\", \"user_2\"]\n",
        "print(f\"Raw user log (with duplicates): {raw_user_log}\")\n",
        "\n",
        "# Convert the list to a set to get a unique list of users.\n",
        "unique_users = set(raw_user_log)\n",
        "print(f\"Unique users from the log: {unique_users}\")\n",
        "\n",
        "# --- Part 2: Set Operations for Comparisons ---\n",
        "# Sets are powerful for comparing different collections of data.\n",
        "\n",
        "print(\"\\n--- Set Operations ---\")\n",
        "# Example: Two sets representing unique users from two different log files.\n",
        "file_1_users = {\"user_1\", \"user_2\", \"user_3\", \"user_5\"}\n",
        "file_2_users = {\"user_3\", \"user_4\", \"user_5\", \"user_6\"}\n",
        "\n",
        "# intersection(): Find common users between the two files.\n",
        "common_users = file_1_users.intersection(file_2_users)\n",
        "print(f\"Users found in both files (common users): {common_users}\")\n",
        "\n",
        "# union(): Combine all unique users from both files.\n",
        "all_unique_users = file_1_users.union(file_2_users)\n",
        "print(f\"All unique users from both files: {all_unique_users}\")\n",
        "\n",
        "# difference(): Find users who are in the first set but not the second.\n",
        "exclusive_to_file_1 = file_1_users.difference(file_2_users)\n",
        "print(f\"Users exclusive to File 1: {exclusive_to_file_1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtEf5gyuMN8I",
        "outputId": "ce72669e-8769-43f6-fff5-4018cbc2a555"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Set Practice: Finding Unique Users ---\n",
            "Raw user log (with duplicates): ['user_1', 'user_2', 'user_1', 'user_3', 'user_4', 'user_2']\n",
            "Unique users from the log: {'user_2', 'user_4', 'user_3', 'user_1'}\n",
            "\n",
            "--- Set Operations ---\n",
            "Users found in both files (common users): {'user_5', 'user_3'}\n",
            "All unique users from both files: {'user_4', 'user_3', 'user_2', 'user_5', 'user_1', 'user_6'}\n",
            "Users exclusive to File 1: {'user_2', 'user_1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Pipeline Mini Project"
      ],
      "metadata": {
        "id": "8HkBTjbqQduv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- User Story ---\n",
        "# As a data engineer, I need to process raw call detail records (CDRs)\n",
        "# from a telecom network to identify valid calls, calculate total call\n",
        "# duration, and count the number of unique subscribers.\n",
        "\n",
        "# The data is messy. I need to:\n",
        "# 1. Remove any entries with missing or invalid data.\n",
        "# 2. Calculate the total call duration in minutes for valid records.\n",
        "# 3. Count the number of unique subscribers.\n",
        "# 4. Generate a summary report with key metrics.\n",
        "\n",
        "# --- Part 1: Raw Data & Initial Setup ---\n",
        "# Our raw data is a list of dictionaries, representing call records.\n",
        "# Some records are missing key information or have incorrect data types.\n",
        "print(\"--- 1. Initializing Raw Call Records ---\")\n",
        "raw_call_records = [\n",
        "    {\"call_id\": \"C001\", \"subscriber_id\": \"S101\", \"duration_minutes\": 5.5, \"status\": \"COMPLETED\"},\n",
        "    {\"call_id\": \"C002\", \"subscriber_id\": \"S102\", \"duration_minutes\": 10.2, \"status\": \"COMPLETED\"},\n",
        "    {\"call_id\": \"C003\", \"subscriber_id\": \"S101\", \"duration_minutes\": 3.0, \"status\": \"COMPLETED\"},\n",
        "    {\"call_id\": \"C004\", \"subscriber_id\": \"S103\", \"duration_minutes\": None, \"status\": \"COMPLETED\"}, # Invalid: duration is None\n",
        "    {\"call_id\": \"C005\", \"subscriber_id\": \"S104\", \"duration_minutes\": 15.0, \"status\": \"FAILED\"},\n",
        "    {\"call_id\": \"C006\", \"subscriber_id\": None, \"duration_minutes\": 8.5, \"status\": \"COMPLETED\"},  # Invalid: subscriber_id is None\n",
        "    {\"call_id\": \"C007\", \"subscriber_id\": \"S105\", \"duration_minutes\": 2.0, \"status\": \"COMPLETED\"},\n",
        "    {\"call_id\": \"C008\", \"subscriber_id\": \"S102\", \"duration_minutes\": \"Unknown\", \"status\": \"COMPLETED\"} # Invalid: duration is a string\n",
        "]\n",
        "print(f\"Total raw records: {len(raw_call_records)}\\n\")\n",
        "\n",
        "\n",
        "# --- Part 2: Data Cleaning Function ---\n",
        "# We'll use a function to clean the data. This demonstrates using\n",
        "# functions, loops, and conditional statements.\n",
        "\n",
        "def clean_data(data):\n",
        "    \"\"\"\n",
        "    Cleans a list of call records by validating each entry.\n",
        "    \"\"\"\n",
        "    cleaned_records = []\n",
        "    invalid_records_count = 0\n",
        "\n",
        "    # Use a for loop to iterate over each record in the data list.\n",
        "    for record in data:\n",
        "        # Check for valid conditions using 'if' and 'and' operators.\n",
        "        # This checks if the 'duration_minutes' and 'subscriber_id' are present and valid.\n",
        "        if (record.get(\"duration_minutes\") is not None and\n",
        "            record.get(\"subscriber_id\") is not None and\n",
        "            isinstance(record.get(\"duration_minutes\"), (int, float))):\n",
        "            cleaned_records.append(record)\n",
        "        else:\n",
        "            invalid_records_count += 1\n",
        "\n",
        "    # Return the clean data and a count of invalid records.\n",
        "    return cleaned_records, invalid_records_count\n",
        "\n",
        "# --- Part 3: Data Processing Function ---\n",
        "# This function calculates key metrics from the cleaned data.\n",
        "# It uses lists, sets, and dictionaries.\n",
        "\n",
        "def process_data(data):\n",
        "    \"\"\"\n",
        "    Processes cleaned call records to generate a summary report.\n",
        "    \"\"\"\n",
        "    total_call_duration = 0.0\n",
        "\n",
        "    # Use a set to automatically handle unique subscriber IDs.\n",
        "    unique_subscribers = set()\n",
        "\n",
        "    # Loop through the cleaned data.\n",
        "    for record in data:\n",
        "        # Add the subscriber ID to our set to find unique subscribers.\n",
        "        unique_subscribers.add(record['subscriber_id'])\n",
        "\n",
        "        # Use an arithmetic operator to sum the call duration.\n",
        "        total_call_duration += record['duration_minutes']\n",
        "\n",
        "    # Create a dictionary to hold our final report.\n",
        "    # This is a perfect use case for key-value pairs.\n",
        "    report = {\n",
        "        \"valid_records_count\": len(data),\n",
        "        \"unique_subscriber_count\": len(unique_subscribers),\n",
        "        \"total_call_duration_minutes\": total_call_duration,\n",
        "        \"average_call_duration\": total_call_duration / len(data) if len(data) > 0 else 0\n",
        "    }\n",
        "\n",
        "    # Return the final report dictionary.\n",
        "    return report\n",
        "\n",
        "# --- Part 4: Main Program Execution ---\n",
        "# This is where the pipeline runs, orchestrating the functions.\n",
        "print(\"--- 2. Cleaning Data... ---\")\n",
        "cleaned_data, invalid_count = clean_data(raw_call_records)\n",
        "print(f\"Removed {invalid_count} invalid records.\")\n",
        "print(f\"Remaining valid records: {len(cleaned_data)}\\n\")\n",
        "\n",
        "print(\"--- 3. Processing Data and Generating Report ---\")\n",
        "final_report = process_data(cleaned_data)\n",
        "\n",
        "# --- Part 5: Final Report ---\n",
        "# Print the results in a readable format.\n",
        "print(\"\\n--- Final Telecom Report ---\")\n",
        "# Use a for loop to iterate through the report dictionary's items.\n",
        "for metric, value in final_report.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"{metric.replace('_', ' ').title()}: {value:,.2f} minutes\")\n",
        "    else:\n",
        "        print(f\"{metric.replace('_', ' ').title()}: {value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr9WeXF7QbOk",
        "outputId": "fb9743b7-4e70-40fa-8047-2f546147b7a8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Initializing Raw Call Records ---\n",
            "Total raw records: 8\n",
            "\n",
            "--- 2. Cleaning Data... ---\n",
            "Removed 3 invalid records.\n",
            "Remaining valid records: 5\n",
            "\n",
            "--- 3. Processing Data and Generating Report ---\n",
            "\n",
            "--- Final Telecom Report ---\n",
            "Valid Records Count: 5\n",
            "Unique Subscriber Count: 4\n",
            "Total Call Duration Minutes: 35.70 minutes\n",
            "Average Call Duration: 7.14 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 6 : Modules & Packages"
      ],
      "metadata": {
        "id": "JIrqcPguVix6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- User Story ---\n",
        "# As a data engineer, I need to list all files in a specific directory\n",
        "# and then read data from a CSV file.\n",
        "\n",
        "# This requires using built-in Python modules.\n",
        "# We will use the 'os' module to interact with the operating system,\n",
        "# and the 'csv' module to handle comma-separated values.\n",
        "\n",
        "# --- Part 1: Using the 'os' Module ---\n",
        "# The 'os' module is a standard library for interacting with the operating system.\n",
        "# It's crucial for any data engineer who works with files and directories.\n",
        "import os\n",
        "\n",
        "# Let's assume our data is in a sub-directory called 'temp_data'.\n",
        "# We can create it if it doesn't exist.\n",
        "directory_path = \"temp_data\"\n",
        "if not os.path.exists(directory_path):\n",
        "    os.makedirs(directory_path)\n",
        "\n",
        "# Print all files and directories in the current working directory.\n",
        "print(\"--- Listing Contents of the Current Directory ---\")\n",
        "# The os.listdir() function returns a list of all entries.\n",
        "contents = os.listdir(\".\")\n",
        "print(f\"Directory contents: {contents}\")\n",
        "\n",
        "# --- Part 2: Creating a Sample CSV File ---\n",
        "# We need a file to read. We'll use a string for now, but in a real scenario,\n",
        "# this would be an actual file on disk.\n",
        "sample_csv_data = \"\"\"id,name,value\n",
        "1,alpha,100\n",
        "2,beta,200\n",
        "3,gamma,150\n",
        "\"\"\"\n",
        "\n",
        "# Let's save this data to a temporary file.\n",
        "temp_file_path = os.path.join(directory_path, \"sample_data.csv\")\n",
        "with open(temp_file_path, \"w\") as f:\n",
        "    f.write(sample_csv_data)\n",
        "print(f\"\\nCreated a temporary CSV file at: {temp_file_path}\")\n",
        "\n",
        "# --- Part 3: Using the 'csv' Module ---\n",
        "# The 'csv' module is part of Python's standard library and is\n",
        "# designed specifically for reading and writing CSV files.\n",
        "import csv\n",
        "import io # We use io.StringIO to treat our string data like a file.\n",
        "\n",
        "# Use a 'with' statement to open and automatically close the file.\n",
        "print(\"\\n--- Reading Data from the CSV File ---\")\n",
        "\n",
        "# In a real-world scenario, you would open the file directly like this:\n",
        "# with open('path/to/file.csv', 'r') as csv_file:\n",
        "#     csv_reader = csv.reader(csv_file)\n",
        "\n",
        "# For this example, we use io.StringIO to read from our in-memory string.\n",
        "csv_reader = csv.reader(io.StringIO(sample_csv_data))\n",
        "\n",
        "# A for loop is perfect for iterating over the rows in the CSV.\n",
        "for row in csv_reader:\n",
        "    # Each row is a list of strings.\n",
        "    print(f\"Row read: {row}\")\n",
        "\n",
        "# Clean up the temporary directory after we're done.\n",
        "os.remove(temp_file_path)\n",
        "os.rmdir(directory_path)\n",
        "print(\"\\nCleaned up temporary directory.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydsLaWjlVofL",
        "outputId": "1555f1c7-140c-4b6c-f262-8836d1c006b1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Listing Contents of the Current Directory ---\n",
            "Directory contents: ['.config', 'temp_data', 'sample_data']\n",
            "\n",
            "Created a temporary CSV file at: temp_data/sample_data.csv\n",
            "\n",
            "--- Reading Data from the CSV File ---\n",
            "Row read: ['id', 'name', 'value']\n",
            "Row read: ['1', 'alpha', '100']\n",
            "Row read: ['2', 'beta', '200']\n",
            "Row read: ['3', 'gamma', '150']\n",
            "\n",
            "Cleaned up temporary directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 7 : String & String Methods"
      ],
      "metadata": {
        "id": "c-0NWAWIXDPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- User Story ---\n",
        "# As a data engineer, I've received a batch of unstructured log data.\n",
        "# Each log entry contains a timestamp, log level, and a message.\n",
        "# I need to clean up this raw data and extract specific pieces of\n",
        "# information for analysis.\n",
        "\n",
        "# We will use various string methods to accomplish this task.\n",
        "\n",
        "# --- Part 1: Sample Log Data ---\n",
        "raw_log_entry = \"  [2023-10-26 08:30:15] INFO:  'User 123' processed 10 records.   \"\n",
        "\n",
        "# --- Part 2: Cleaning the String ---\n",
        "# We need to remove any leading or trailing whitespace.\n",
        "# The .strip() method is perfect for this.\n",
        "cleaned_log_entry = raw_log_entry.strip()\n",
        "print(\"--- Cleaning the Raw String ---\")\n",
        "print(f\"Raw: '{raw_log_entry}'\")\n",
        "print(f\"Cleaned: '{cleaned_log_entry}'\\n\")\n",
        "\n",
        "# --- Part 3: Finding and Slicing ---\n",
        "# We want to isolate the log level ('INFO').\n",
        "# The .find() method returns the index of a substring.\n",
        "# Slicing allows us to extract a part of the string.\n",
        "start_index = cleaned_log_entry.find(\"INFO:\")\n",
        "end_index = cleaned_log_entry.find(\":\", start_index) + 1\n",
        "log_level = cleaned_log_entry[start_index:end_index]\n",
        "print(\"--- Finding & Slicing ---\")\n",
        "print(f\"Extracted Log Level: '{log_level}'\\n\")\n",
        "\n",
        "# --- Part 4: Splitting the String ---\n",
        "# We want to separate the log entry into parts (e.g., timestamp, level, message).\n",
        "# The .split() method is ideal for this.\n",
        "# Let's split by the colon (':').\n",
        "parts = cleaned_log_entry.split(':')\n",
        "print(\"--- Splitting the String ---\")\n",
        "print(f\"Split parts: {parts}\")\n",
        "# The third part contains the message, but it has extra space.\n",
        "log_message = parts[2].strip()\n",
        "print(f\"Extracted Log Message: '{log_message}'\\n\")\n",
        "\n",
        "# --- Part 5: Replacing and Joining ---\n",
        "# Let's say we want to anonymize the user ID and standardize the string.\n",
        "# The .replace() method helps us replace a substring.\n",
        "anonymous_message = log_message.replace('User 123', 'ANONYMOUS_USER')\n",
        "print(\"--- Replacing a Substring ---\")\n",
        "print(f\"Anonymized Message: '{anonymous_message}'\\n\")\n",
        "\n",
        "# Now, let's say we need to join the parts back together with a different separator.\n",
        "# The .join() method is used for this.\n",
        "# It takes an iterable (like a list) and concatenates its elements into a string.\n",
        "new_separator = \" | \"\n",
        "reconstructed_string = new_separator.join(parts)\n",
        "print(\"--- Joining Strings ---\")\n",
        "print(f\"Reconstructed String: '{reconstructed_string}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwjrzWNhXGcv",
        "outputId": "bc6cb39d-ed93-4881-99ac-b416f868b0dd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Cleaning the Raw String ---\n",
            "Raw: '  [2023-10-26 08:30:15] INFO:  'User 123' processed 10 records.   '\n",
            "Cleaned: '[2023-10-26 08:30:15] INFO:  'User 123' processed 10 records.'\n",
            "\n",
            "--- Finding & Slicing ---\n",
            "Extracted Log Level: 'INFO:'\n",
            "\n",
            "--- Splitting the String ---\n",
            "Split parts: ['[2023-10-26 08', '30', '15] INFO', \"  'User 123' processed 10 records.\"]\n",
            "Extracted Log Message: '15] INFO'\n",
            "\n",
            "--- Replacing a Substring ---\n",
            "Anonymized Message: '15] INFO'\n",
            "\n",
            "--- Joining Strings ---\n",
            "Reconstructed String: '[2023-10-26 08 | 30 | 15] INFO |   'User 123' processed 10 records.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Telecom Data Analysis"
      ],
      "metadata": {
        "id": "CHpAyQTOYy7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- User Story ---\n",
        "# As a data engineer for a telecom company, I've received a batch of call records\n",
        "# from the past hour. My task is to process this raw data to provide a quick\n",
        "# summary for the business. This summary should include the total number of calls,\n",
        "# the number of failed calls, and a list of the top callers.\n",
        "\n",
        "# This project demonstrates the use of variables, data types, lists, dictionaries,\n",
        "# loops, conditionals, functions, and sets.\n",
        "\n",
        "# --- Part 1: Simulate Raw Data ---\n",
        "# In a real-world scenario, this data would come from a database or a file.\n",
        "# For this project, we'll simulate a list of dictionaries.\n",
        "raw_call_records = [\n",
        "    {\n",
        "        'call_id': 'c101',\n",
        "        'from': '555-1234',\n",
        "        'to': '555-5678',\n",
        "        'duration_seconds': 120,\n",
        "        'status': 'success'\n",
        "    },\n",
        "    {\n",
        "        'call_id': 'c102',\n",
        "        'from': '555-1234',\n",
        "        'to': '555-8888',\n",
        "        'duration_seconds': 45,\n",
        "        'status': 'success'\n",
        "    },\n",
        "    {\n",
        "        'call_id': 'c103',\n",
        "        'from': '555-9999',\n",
        "        'to': '555-1111',\n",
        "        'duration_seconds': 0,\n",
        "        'status': 'failed'\n",
        "    },\n",
        "    {\n",
        "        'call_id': 'c104',\n",
        "        'from': '555-1234',\n",
        "        'to': '555-2222',\n",
        "        'duration_seconds': 90,\n",
        "        'status': 'success'\n",
        "    },\n",
        "    {\n",
        "        'call_id': 'c105',\n",
        "        'from': '555-9999',\n",
        "        'to': '555-4444',\n",
        "        'duration_seconds': 30,\n",
        "        'status': 'success'\n",
        "    },\n",
        "    {\n",
        "        'call_id': 'c106',\n",
        "        'from': '555-3333',\n",
        "        'to': '555-5555',\n",
        "        'duration_seconds': 0,\n",
        "        'status': 'failed'\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Part 2: Process the Data with a Function ---\n",
        "# Encapsulating our logic in a function makes our code reusable.\n",
        "def analyze_call_records(records):\n",
        "    \"\"\"\n",
        "    Analyzes a list of call records to generate a summary report.\n",
        "    \"\"\"\n",
        "    # Initialize variables to hold our metrics.\n",
        "    total_calls = 0\n",
        "    success_calls = 0\n",
        "    failed_calls = 0\n",
        "    total_duration = 0\n",
        "\n",
        "    # Use a dictionary to count calls from each number.\n",
        "    caller_counts = {}\n",
        "\n",
        "    # We will use a set to find the number of unique callers.\n",
        "    unique_callers = set()\n",
        "\n",
        "    # Use a for loop to iterate over each record in the list.\n",
        "    for call in records:\n",
        "        total_calls += 1\n",
        "\n",
        "        # Use conditional statements to check the status.\n",
        "        if call['status'] == 'success':\n",
        "            success_calls += 1\n",
        "            total_duration += call['duration_seconds']\n",
        "        elif call['status'] == 'failed':\n",
        "            failed_calls += 1\n",
        "\n",
        "        # Update the caller counts using a dictionary.\n",
        "        caller = call['from']\n",
        "        if caller in caller_counts:\n",
        "            caller_counts[caller] += 1\n",
        "        else:\n",
        "            caller_counts[caller] = 1\n",
        "\n",
        "        # Add the caller to the set of unique callers.\n",
        "        unique_callers.add(caller)\n",
        "\n",
        "    # Use a dictionary to structure our final report.\n",
        "    report = {\n",
        "        'total_calls': total_calls,\n",
        "        'success_rate_percent': (success_calls / total_calls) * 100 if total_calls > 0 else 0,\n",
        "        'failed_calls': failed_calls,\n",
        "        'total_duration_minutes': total_duration / 60,\n",
        "        'unique_callers': len(unique_callers),\n",
        "        'caller_breakdown': caller_counts\n",
        "    }\n",
        "\n",
        "    return report\n",
        "\n",
        "# --- Part 3: Run the Analysis and Print the Report ---\n",
        "call_report = analyze_call_records(raw_call_records)\n",
        "\n",
        "print(\"--- Telecom Call Analysis Report ---\")\n",
        "print(f\"Total Calls Processed: {call_report['total_calls']}\")\n",
        "print(f\"Total Failed Calls: {call_report['failed_calls']}\")\n",
        "print(f\"Success Rate: {call_report['success_rate_percent']:.2f}%\")\n",
        "print(f\"Total Talk Time: {call_report['total_duration_minutes']:.2f} minutes\")\n",
        "print(f\"Number of Unique Callers: {call_report['unique_callers']}\")\n",
        "\n",
        "print(\"\\n--- Caller Breakdown ---\")\n",
        "# Use a for loop to iterate over the items in the caller_breakdown dictionary.\n",
        "for caller, count in call_report['caller_breakdown'].items():\n",
        "    print(f\"  Caller {caller}: {count} calls\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK3VpGz-Y2Cl",
        "outputId": "8d5258c6-8816-4624-b9d9-8adf7b90d54f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Telecom Call Analysis Report ---\n",
            "Total Calls Processed: 6\n",
            "Total Failed Calls: 2\n",
            "Success Rate: 66.67%\n",
            "Total Talk Time: 4.75 minutes\n",
            "Number of Unique Callers: 3\n",
            "\n",
            "--- Caller Breakdown ---\n",
            "  Caller 555-1234: 3 calls\n",
            "  Caller 555-9999: 2 calls\n",
            "  Caller 555-3333: 1 calls\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 9: Lists & List Operations"
      ],
      "metadata": {
        "id": "ym-sHnhZMh4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User Story: As a data engineer, I need to process a batch of customer orders.\n",
        "# I need to add new orders, remove invalid ones, and perform quick checks on the\n",
        "# data before it's moved to the next stage of the pipeline.\n",
        "\n",
        "# --- Part 1: Initializing and Modifying Lists ---\n",
        "# Lists are ordered, mutable collections of items. You can change them at\n",
        "# any time, which makes them highly versatile.\n",
        "\n",
        "print(\"--- Initializing and Modifying Lists ---\")\n",
        "# Example: A list of customer orders to be processed.\n",
        "# Each order is represented by its unique ID.\n",
        "customer_orders = [\"ORD_101\", \"ORD_102\", \"ORD_103\"]\n",
        "print(f\"Original list of customer orders: {customer_orders}\")\n",
        "\n",
        "# We received a new order. Let's add it to the list.\n",
        "customer_orders.append(\"ORD_104\")\n",
        "print(f\"List after appending a new order: {customer_orders}\")\n",
        "\n",
        "# A customer canceled an order. Let's remove it.\n",
        "customer_orders.remove(\"ORD_102\")\n",
        "print(f\"List after removing a canceled order: {customer_orders}\")\n",
        "\n",
        "# A special case: sometimes you need to add multiple items at once.\n",
        "new_orders = [\"ORD_105\", \"ORD_106\"]\n",
        "customer_orders.extend(new_orders)\n",
        "print(f\"List after extending with a new batch of orders: {customer_orders}\")\n",
        "\n",
        "# --- Part 2: Accessing and Slicing Lists ---\n",
        "# Accessing elements is straightforward with indexing, and slicing\n",
        "# allows you to work with a subset of your data.\n",
        "\n",
        "print(\"\\n--- Accessing and Slicing Lists ---\")\n",
        "# Access the first order in the list.\n",
        "first_order = customer_orders[0]\n",
        "print(f\"The first order to be processed is: {first_order}\")\n",
        "\n",
        "# Access the last order using a negative index.\n",
        "last_order = customer_orders[-1]\n",
        "print(f\"The last order is: {last_order}\")\n",
        "\n",
        "# Use slicing to get a range of orders. This is useful for\n",
        "# processing data in smaller chunks.\n",
        "processed_batch = customer_orders[1:4]  # Gets items from index 1 up to (but not including) 4\n",
        "print(f\"A batch of orders for processing: {processed_batch}\")\n",
        "\n",
        "# --- Part 3: List Functions & Other Operations ---\n",
        "# There are many built-in functions and methods that are essential for\n",
        "# common data engineering tasks.\n",
        "\n",
        "print(\"\\n--- List Functions and Operations ---\")\n",
        "# Get the total number of orders in the list.\n",
        "print(f\"Total number of orders to process: {len(customer_orders)}\")\n",
        "\n",
        "# Check if a specific order exists in the list.\n",
        "is_order_103_present = \"ORD_103\" in customer_orders\n",
        "print(f\"Is order 'ORD_103' in the list? {is_order_103_present}\")\n",
        "\n",
        "# Find the index of an item.\n",
        "try:\n",
        "    index_of_order_104 = customer_orders.index(\"ORD_104\")\n",
        "    print(f\"Order 'ORD_104' is located at index: {index_of_order_104}\")\n",
        "except ValueError:\n",
        "    print(\"Order 'ORD_104' was not found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m_HydstMjqJ",
        "outputId": "e799954f-3256-4bdb-c03d-f26d97a05ac4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initializing and Modifying Lists ---\n",
            "Original list of customer orders: ['ORD_101', 'ORD_102', 'ORD_103']\n",
            "List after appending a new order: ['ORD_101', 'ORD_102', 'ORD_103', 'ORD_104']\n",
            "List after removing a canceled order: ['ORD_101', 'ORD_103', 'ORD_104']\n",
            "List after extending with a new batch of orders: ['ORD_101', 'ORD_103', 'ORD_104', 'ORD_105', 'ORD_106']\n",
            "\n",
            "--- Accessing and Slicing Lists ---\n",
            "The first order to be processed is: ORD_101\n",
            "The last order is: ORD_106\n",
            "A batch of orders for processing: ['ORD_103', 'ORD_104', 'ORD_105']\n",
            "\n",
            "--- List Functions and Operations ---\n",
            "Total number of orders to process: 5\n",
            "Is order 'ORD_103' in the list? True\n",
            "Order 'ORD_104' is located at index: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 10: Dictionaries & Dictionary Methods."
      ],
      "metadata": {
        "id": "sXww17wJNs7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User Story: As a data engineer, I've received a batch of log entries.\n",
        "# I need to parse each log entry to extract specific information like the\n",
        "# event type, server ID, and timestamp and store it in a structured format.\n",
        "\n",
        "# --- Part 1: Creating and Accessing Dictionaries ---\n",
        "# Dictionaries store data in key-value pairs. Keys must be unique and\n",
        "# immutable (like strings, numbers, or tuples).\n",
        "\n",
        "print(\"--- Creating and Accessing Dictionaries ---\")\n",
        "# Example: A dictionary representing a single log entry.\n",
        "log_entry = {\n",
        "    \"timestamp\": \"2024-05-18T12:00:00Z\",\n",
        "    \"event_type\": \"Data_Ingestion_Start\",\n",
        "    \"server_id\": \"server-101\",\n",
        "    \"status\": \"SUCCESS\"\n",
        "}\n",
        "\n",
        "# Access a value using its key.\n",
        "print(f\"The event type is: {log_entry['event_type']}\")\n",
        "print(f\"The timestamp is: {log_entry['timestamp']}\")\n",
        "\n",
        "# You can also add new key-value pairs to the dictionary.\n",
        "log_entry[\"run_duration_ms\"] = 1500\n",
        "print(f\"Log entry with new data: {log_entry}\")\n",
        "\n",
        "# --- Part 2: Dictionary Methods ---\n",
        "# Dictionaries have useful methods for interacting with their keys and values.\n",
        "\n",
        "print(\"\\n--- Exploring Dictionary Methods ---\")\n",
        "# Get a list of all keys using the .keys() method.\n",
        "keys = log_entry.keys()\n",
        "print(f\"All keys in the log entry: {list(keys)}\")\n",
        "\n",
        "# Get a list of all values using the .values() method.\n",
        "values = log_entry.values()\n",
        "print(f\"All values in the log entry: {list(values)}\")\n",
        "\n",
        "# Check if a key exists in the dictionary using the 'in' operator.\n",
        "# This is a critical check for preventing errors.\n",
        "is_run_duration_present = \"run_duration_ms\" in log_entry\n",
        "print(f\"Is 'run_duration_ms' key present? {is_run_duration_present}\")\n",
        "\n",
        "is_user_id_present = \"user_id\" in log_entry\n",
        "print(f\"Is 'user_id' key present? {is_user_id_present}\")\n",
        "\n",
        "# The .get() method is a safe way to access a value. It returns None if\n",
        "# the key is not found, preventing a KeyError.\n",
        "status = log_entry.get(\"status\")\n",
        "print(f\"The status is: {status}\")\n",
        "\n",
        "user_id = log_entry.get(\"user_id\", \"NOT_FOUND\")\n",
        "print(f\"The user ID is: {user_id}\")\n",
        "\n",
        "# --- Part 3: Deleting Data ---\n",
        "# You can remove key-value pairs from a dictionary using del.\n",
        "print(\"\\n--- Deleting Dictionary Items ---\")\n",
        "print(f\"Original dictionary before deletion: {log_entry}\")\n",
        "del log_entry[\"status\"]\n",
        "print(f\"Dictionary after deleting 'status': {log_entry}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaSlRpZaNukB",
        "outputId": "3da8c72e-0b38-4241-c5c7-c2620fe8bced"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Creating and Accessing Dictionaries ---\n",
            "The event type is: Data_Ingestion_Start\n",
            "The timestamp is: 2024-05-18T12:00:00Z\n",
            "Log entry with new data: {'timestamp': '2024-05-18T12:00:00Z', 'event_type': 'Data_Ingestion_Start', 'server_id': 'server-101', 'status': 'SUCCESS', 'run_duration_ms': 1500}\n",
            "\n",
            "--- Exploring Dictionary Methods ---\n",
            "All keys in the log entry: ['timestamp', 'event_type', 'server_id', 'status', 'run_duration_ms']\n",
            "All values in the log entry: ['2024-05-18T12:00:00Z', 'Data_Ingestion_Start', 'server-101', 'SUCCESS', 1500]\n",
            "Is 'run_duration_ms' key present? True\n",
            "Is 'user_id' key present? False\n",
            "The status is: SUCCESS\n",
            "The user ID is: NOT_FOUND\n",
            "\n",
            "--- Deleting Dictionary Items ---\n",
            "Original dictionary before deletion: {'timestamp': '2024-05-18T12:00:00Z', 'event_type': 'Data_Ingestion_Start', 'server_id': 'server-101', 'status': 'SUCCESS', 'run_duration_ms': 1500}\n",
            "Dictionary after deleting 'status': {'timestamp': '2024-05-18T12:00:00Z', 'event_type': 'Data_Ingestion_Start', 'server_id': 'server-101', 'run_duration_ms': 1500}\n"
          ]
        }
      ]
    }
  ]
}